name: Merge TVBox Sources

on:
  push:
    paths:
      - 'é’é¾™é¢æ¿ç‰ˆæ‰€ç”¨å·¥å…·/source.json'
  schedule:
    - cron: '0 0 * * *'
  workflow_dispatch:

jobs:
  merge-sources:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: pip install requests
      
    - name: Create and run merge script
      run: |
        cat > merge_script.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import sys
import json
import time
import requests

# ======================
# é…ç½®åŒº - ä¿®æ”¹ä¸ºç›¸å¯¹è·¯å¾„
# ======================
SOURCES_JSON_PATH = 'é’é¾™é¢æ¿ç‰ˆæ‰€ç”¨å·¥å…·/source.json'
TARGET_JSON_PATH = 'QLTV.json'

# è¯·æ±‚å¤´
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (compatible; TVBoxMerge/1.0)'
}

def get_sites_from_url(url):
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        if resp.status_code == 200:
            content = resp.text
            try:
                data = json.loads(content)
                if isinstance(data, dict) and 'sites' in data:
                    return data['sites']
                elif isinstance(data, list):
                    return data
            except json.JSONDecodeError:
                start = content.find('{')
                end = content.rfind('}') + 1
                if start != -1 and end != 0:
                    data = json.loads(content[start:end])
                    if isinstance(data, dict) and 'sites' in data:
                        return data['sites']
        return []
    except Exception as e:
        print(f"[è¯·æ±‚å¤±è´¥] {url} | {e}")
        return []

def fix_site_paths(site, base_url, jar_url):
    base = base_url.rstrip('/')
    for k, v in site.items():
        if isinstance(v, str) and v.startswith('./'):
            site[k] = base + '/' + v[2:]
    if 'jar' not in site and jar_url and jar_url.strip():
        site['jar'] = jar_url.rstrip('/')
    return site

def main():
    if not os.path.exists(SOURCES_JSON_PATH):
        print(f"[é”™è¯¯] æ‰¾ä¸åˆ°æºé…ç½®ï¼š{SOURCES_JSON_PATH}")
        return

    try:
        with open(SOURCES_JSON_PATH, 'r', encoding='utf-8') as f:
            sources = json.load(f)
    except Exception as e:
        print(f"[é”™è¯¯] è¯»å– sources.json å¤±è´¥ï¼š{e}")
        return

    if os.path.exists(TARGET_JSON_PATH):
        try:
            with open(TARGET_JSON_PATH, 'r', encoding='utf-8') as f:
                target_data = json.load(f)
        except Exception as e:
            print(f"[è­¦å‘Š] è¯»å–ç›®æ ‡æ–‡ä»¶å¤±è´¥ï¼Œå°†æ–°å»ºï¼š{e}")
            target_data = {}
    else:
        target_data = {}

    if not isinstance(target_data, dict):
        print("[è­¦å‘Š] ç›®æ ‡æ–‡ä»¶å†…å®¹ä¸æ˜¯å¯¹è±¡ï¼Œå°†ä½¿ç”¨ç©ºå¯¹è±¡ä½œä¸ºåŸºç¡€")
        target_data = {}

    if 'sites' not in target_data:
        target_data['sites'] = []
    target_data['sites'][:] = []

    seen_keys = set()
    merged_sites = []
    for src in sources:
        url = src.get('url')
        jar = src.get('jar') or ''
        base = src.get('base') or ''

        if not url:
            continue

        print(f"[æ‹‰å–] {url}")
        sites = get_sites_from_url(url)
        base_url = base.rstrip('/') + '/'
        jar_url = jar.rstrip('/')
        for s in sites:
            fixed = fix_site_paths(s, base_url, jar_url)
            key = fixed.get('key', '').strip()
            if key and key not in seen_keys:
                seen_keys.add(key)
                merged_sites.append(fixed)

    target_data['sites'] = merged_sites

    try:
        with open(TARGET_JSON_PATH, 'w', encoding='utf-8') as f:
            items = []
            for k in sorted(target_data.keys()):
                v = target_data[k]
                if k == 'sites':
                    if not merged_sites:
                        items.append('  "sites": []')
                    else:
                        lines = [json.dumps(x, ensure_ascii=False, separators=(',', ':')) for x in merged_sites]
                        joined = ',\n    '.join(lines)
                        items.append(f'  "sites": [\n    {joined}\n  ]')
                else:
                    items.append(f'  "{k}": {json.dumps(v, ensure_ascii=False, separators=(",", ":"))}')
            f.write('{\n')
            f.write(',\n'.join(items))
            f.write('\n}\n')

        print(f"[å®Œæˆ] å·²ç”Ÿæˆï¼š{TARGET_JSON_PATH}ï¼Œå…± {len(merged_sites)} ä¸ªç«™ç‚¹")
    except Exception as e:
        print(f"[é”™è¯¯] å†™å‡ºæ–‡ä»¶å¤±è´¥ï¼š{e}")

if __name__ == '__main__':
    main()
EOF

        python merge_script.py
        
    - name: Commit and push if changed
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add -A
        git diff --staged --quiet || (git commit -m "ðŸ¤– Auto update TVBox sources" && git push)
